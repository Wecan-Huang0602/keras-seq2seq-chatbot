{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "\n",
    "# 为语料做分词处理\n",
    "def word_segment():\n",
    "    # 打开语料文本\n",
    "    inputFile_NoSegment = open(os.getcwd() + '/對話文本/Happy.txt', 'rb')\n",
    "    outputFile_Segment = open(os.getcwd() + '/對話文本/Happy_segment.txt',\n",
    "                              'w', encoding='utf-8')\n",
    "\n",
    "    # 读取语料文本中的每一行文字\n",
    "    lines = inputFile_NoSegment.readlines()\n",
    "\n",
    "    # 为每一行文字分词\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i]\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            seg_list = jieba.cut(line)\n",
    "\n",
    "            segments = ''\n",
    "            for word in seg_list:\n",
    "                segments = segments + ' ' + word\n",
    "            segments += '\\n'\n",
    "            segments = segments.lstrip()\n",
    "\n",
    "            # 将分词后的语句，写进文件中\n",
    "            outputFile_Segment.write(segments)\n",
    "\n",
    "    inputFile_NoSegment.close()\n",
    "    outputFile_Segment.close()\n",
    "\n",
    "word_segment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 讀入斷詞後的文本\n",
    "text_Segment = open(os.getcwd() + '/對話文本/Happy_segment.txt',\n",
    "                    'r', encoding='utf-8')\n",
    "text_Segment_list = text_Segment.readlines()\n",
    "text_Segment.close()\n",
    "\n",
    "# 移除文本內的'\\n'\n",
    "text_Segment_list = [n.rstrip() for n in text_Segment_list]\n",
    "\n",
    "print('對話文本筆數：', len(text_Segment_list))\n",
    "print('對話文本內容：', text_Segment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MAX_INPUT_SEQ_LENGTH = 10\n",
    "MAX_TARGET_SEQ_LENGTH = 15\n",
    "MAX_INPUT_VOCAB_SIZE = 1500\n",
    "MAX_TARGET_VOCAB_SIZE = 1500\n",
    "\n",
    "\n",
    "def fit_text(X, Y, input_seq_max_length=None, target_seq_max_length=None):\n",
    "    if input_seq_max_length is None:\n",
    "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "    if target_seq_max_length is None:\n",
    "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "    input_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    max_input_seq_length = 0\n",
    "    max_target_seq_length = 0\n",
    "\n",
    "    for line in X:\n",
    "        text = [word.lower() for word in line.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > input_seq_max_length:\n",
    "            text = text[0:input_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            input_counter[word] += 1\n",
    "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "\n",
    "    for line in Y:\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        text = [word for word in line2.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > target_seq_max_length:\n",
    "            text = text[0:target_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            target_counter[word] += 1\n",
    "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
    "\n",
    "    input_word2idx = dict()\n",
    "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "        input_word2idx[word[0]] = idx + 2\n",
    "    input_word2idx['PAD'] = 0\n",
    "    input_word2idx['UNK'] = 1\n",
    "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "    target_word2idx = dict()\n",
    "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "        target_word2idx[word[0]] = idx + 1\n",
    "    target_word2idx['UNK'] = 0\n",
    "\n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "    \n",
    "    num_input_tokens = len(input_word2idx)\n",
    "    num_target_tokens = len(target_word2idx)\n",
    "\n",
    "    config = dict()\n",
    "    config['input_word2idx'] = input_word2idx\n",
    "    config['input_idx2word'] = input_idx2word\n",
    "    config['target_word2idx'] = target_word2idx\n",
    "    config['target_idx2word'] = target_idx2word\n",
    "    config['num_input_tokens'] = num_input_tokens\n",
    "    config['num_target_tokens'] = num_target_tokens\n",
    "    config['max_input_seq_length'] = max_input_seq_length\n",
    "    config['max_target_seq_length'] = max_target_seq_length\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_Segment_list[0:][::2] # 使用者說的話\n",
    "Y = text_Segment_list[1:][::2] # 聊天機器人的回應\n",
    "config = fit_text(X, Y)\n",
    "\n",
    "print('num_input_tokens: ', config['num_input_tokens'])\n",
    "print('num_target_tokens: ', config['num_target_tokens'])\n",
    "print('max_input_seq_length: ', config['max_input_seq_length'])\n",
    "print('max_target_seq_length: ', config['max_target_seq_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "HIDDEN_UNITS = 100\n",
    "DEFAULT_BATCH_SIZE = 64\n",
    "VERBOSE = 1\n",
    "DEFAULT_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "class Seq2SeqSummarizer(object):\n",
    "\n",
    "    model_name = 'seq2seq'\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.num_input_tokens = config['num_input_tokens']\n",
    "        self.max_input_seq_length = config['max_input_seq_length']\n",
    "        self.num_target_tokens = config['num_target_tokens']\n",
    "        self.max_target_seq_length = config['max_target_seq_length']\n",
    "        self.input_word2idx = config['input_word2idx']\n",
    "        self.input_idx2word = config['input_idx2word']\n",
    "        self.target_word2idx = config['target_word2idx']\n",
    "        self.target_idx2word = config['target_idx2word']\n",
    "        self.config = config\n",
    "\n",
    "        self.version = 0\n",
    "        if 'version' in config:\n",
    "            self.version = config['version']\n",
    "\n",
    "        encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "        encoder_embedding = Embedding(input_dim=self.num_input_tokens, output_dim=HIDDEN_UNITS,\n",
    "                                      input_length=self.max_input_seq_length, name='encoder_embedding')\n",
    "        encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "        encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "        encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "        decoder_inputs = Input(shape=(None, self.num_target_tokens), name='decoder_inputs')\n",
    "        decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "        decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
    "                                                                         initial_state=encoder_states)\n",
    "        decoder_dense = Dense(units=self.num_target_tokens, activation='softmax', name='decoder_dense')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        plot_model(model, to_file='model.png',show_shapes=True)\n",
    "        \n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=LEARNING_RATE), metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.encoder_model = Model(encoder_inputs, encoder_states)\n",
    "        plot_model(self.encoder_model, to_file='self.encoder_model.png',show_shapes=True)\n",
    "\n",
    "        decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "        plot_model(self.decoder_model, to_file='self.decoder_model.png',show_shapes=True)\n",
    "\n",
    "    def load_weights(self, weight_file_path):\n",
    "        if os.path.exists(weight_file_path):\n",
    "            self.model.load_weights(weight_file_path)\n",
    "\n",
    "    def transform_input_text(self, texts):\n",
    "        temp = []\n",
    "        for line in texts:\n",
    "            x = []\n",
    "            for word in line.lower().split(' '):\n",
    "                wid = 1\n",
    "                if word in self.input_word2idx:\n",
    "                    wid = self.input_word2idx[word]\n",
    "                x.append(wid)\n",
    "                if len(x) >= self.max_input_seq_length:\n",
    "                    break\n",
    "            temp.append(x)\n",
    "        temp = pad_sequences(temp, maxlen=self.max_input_seq_length)\n",
    "\n",
    "        print(temp.shape)\n",
    "        return temp\n",
    "\n",
    "    def transform_target_encoding(self, texts):\n",
    "        temp = []\n",
    "        for line in texts:\n",
    "            x = []\n",
    "            line2 = 'START ' + line.lower() + ' END'\n",
    "            for word in line2.split(' '):\n",
    "                x.append(word)\n",
    "                if len(x) >= self.max_target_seq_length:\n",
    "                    break\n",
    "            temp.append(x)\n",
    "\n",
    "        temp = np.array(temp)\n",
    "        print(temp.shape)\n",
    "        return temp\n",
    "\n",
    "    def generate_batch(self, x_samples, y_samples, batch_size):\n",
    "        num_batches = len(x_samples) // batch_size\n",
    "        while True:\n",
    "            for batchIdx in range(0, num_batches):\n",
    "                start = batchIdx * batch_size\n",
    "                end = (batchIdx + 1) * batch_size\n",
    "                encoder_input_data_batch = pad_sequences(x_samples[start:end], self.max_input_seq_length)\n",
    "                decoder_target_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length, self.num_target_tokens))\n",
    "                decoder_input_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length, self.num_target_tokens))\n",
    "                for lineIdx, target_words in enumerate(y_samples[start:end]):\n",
    "                    for idx, w in enumerate(target_words):\n",
    "                        w2idx = 0  # default [UNK]\n",
    "                        if w in self.target_word2idx:\n",
    "                            w2idx = self.target_word2idx[w]\n",
    "                        if w2idx != 0:\n",
    "                            decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
    "                            if idx > 0:\n",
    "                                decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "                yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def get_weight_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqSummarizer.model_name + '-weights.h5'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqSummarizer.model_name + '-config.npy'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_architecture_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqSummarizer.model_name + '-architecture.json'\n",
    "\n",
    "    def fit(self, Xtrain, Ytrain, Xtest, Ytest, epochs=None, batch_size=None, model_dir_path=None):\n",
    "        if epochs is None:\n",
    "            epochs = DEFAULT_EPOCHS\n",
    "        if model_dir_path is None:\n",
    "            model_dir_path = './models'\n",
    "        if batch_size is None:\n",
    "            batch_size = DEFAULT_BATCH_SIZE\n",
    "\n",
    "        self.version += 1\n",
    "        self.config['version'] = self.version\n",
    "        config_file_path = Seq2SeqSummarizer.get_config_file_path(model_dir_path)\n",
    "        weight_file_path = Seq2SeqSummarizer.get_weight_file_path(model_dir_path)\n",
    "        checkpoint = ModelCheckpoint(weight_file_path)\n",
    "        np.save(config_file_path, self.config)\n",
    "        architecture_file_path = Seq2SeqSummarizer.get_architecture_file_path(model_dir_path)\n",
    "        open(architecture_file_path, 'w').write(self.model.to_json())\n",
    "\n",
    "        Ytrain = self.transform_target_encoding(Ytrain)\n",
    "        Ytest = self.transform_target_encoding(Ytest)\n",
    "\n",
    "        Xtrain = self.transform_input_text(Xtrain)\n",
    "        Xtest = self.transform_input_text(Xtest)\n",
    "\n",
    "        train_gen = self.generate_batch(Xtrain, Ytrain, batch_size)\n",
    "        test_gen = self.generate_batch(Xtest, Ytest, batch_size)\n",
    "\n",
    "        train_num_batches = len(Xtrain) // batch_size\n",
    "        test_num_batches = len(Xtest) // batch_size\n",
    "\n",
    "        history = self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                           epochs=epochs,\n",
    "                                           verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                                           callbacks=[checkpoint])\n",
    "        self.model.save_weights(weight_file_path)\n",
    "        return history\n",
    "\n",
    "    def summarize(self, input_text):\n",
    "        input_seq = []\n",
    "        input_wids = []\n",
    "        for word in input_text.lower().split(' '):\n",
    "            idx = 1  # default [UNK]\n",
    "            if word in self.input_word2idx:\n",
    "                idx = self.input_word2idx[word]\n",
    "            input_wids.append(idx)\n",
    "        input_seq.append(input_wids)\n",
    "        input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n",
    "        states_value = self.encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
    "        target_seq[0, 0, self.target_word2idx['START']] = 1\n",
    "        target_text = ''\n",
    "        target_text_len = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            #print (target_seq.shape)\n",
    "            #print (np.array(states_value).shape)\n",
    "            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
    "            #print (np.array(output_tokens).shape)\n",
    "            #print (np.array(h).shape)\n",
    "            #print (np.array(c).shape)\n",
    "\n",
    "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "            sample_word = self.target_idx2word[sample_token_idx]\n",
    "            target_text_len += 1\n",
    "\n",
    "            if sample_word != 'START' and sample_word != 'END':\n",
    "                target_text += ' ' + sample_word\n",
    "\n",
    "            if sample_word == 'END' or target_text_len >= self.max_target_seq_length:\n",
    "                terminated = True\n",
    "\n",
    "            target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
    "            target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "            states_value = [h, c]\n",
    "        return target_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%notebook inline\n",
    "\n",
    "def create_history_plot(history, model_name, metrics=None):\n",
    "    plt.title('Accuracy and Loss (' + model_name + ')')\n",
    "    if metrics is None:\n",
    "        metrics = {'acc', 'loss'}\n",
    "    if 'acc' in metrics:\n",
    "        plt.plot(history.history['accuracy'], color='g', label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], color='b', label='Validation Accuracy')\n",
    "    if 'loss' in metrics:\n",
    "        plt.plot(history.history['loss'], color='r', label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], color='m', label='Validation Loss')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def plot_history(history, model_name):\n",
    "    create_history_plot(history, model_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_and_save_history(history, model_name, file_path, metrics=None):\n",
    "    if metrics is None:\n",
    "        metrics = {'acc', 'loss'}\n",
    "    create_history_plot(history, model_name, metrics)\n",
    "    plt.savefig(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "report_dir_path = './reports'\n",
    "model_dir_path = './models'\n",
    "\n",
    "if not os.path.exists(report_dir_path):\n",
    "    os.mkdir(report_dir_path)\n",
    "    \n",
    "if not os.path.exists(model_dir_path):\n",
    "    os.mkdir(model_dir_path)\n",
    "\n",
    "summarizer = Seq2SeqSummarizer(config)\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('training size: ', len(Xtrain))\n",
    "print('testing size: ', len(Xtest))\n",
    "\n",
    "print('start fitting ...')\n",
    "history = summarizer.fit(Xtrain, Ytrain, Xtest, Ytest, epochs=200)\n",
    "\n",
    "history_plot_file_path = report_dir_path + '/' + Seq2SeqSummarizer.model_name + '-history.png'\n",
    "plot_and_save_history(history, summarizer.model_name, history_plot_file_path, metrics={'loss', 'acc'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model_dir_path = './models'\n",
    "config = np.load(Seq2SeqSummarizer.get_config_file_path(model_dir_path=model_dir_path), allow_pickle=True).item()\n",
    "\n",
    "summarizer = Seq2SeqSummarizer(config)\n",
    "summarizer.load_weights(weight_file_path=Seq2SeqSummarizer.get_weight_file_path(model_dir_path=model_dir_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start predicting ...')\n",
    "for i in np.random.permutation(np.arange(len(X)))[0:10]:\n",
    "    x = X[i]\n",
    "    actual_response = Y[i]\n",
    "    response = summarizer.summarize(x)\n",
    "    print('使用者說的話: ', x)\n",
    "    print('聊天機器人的回應: ', response)\n",
    "    print('文本內正確的回應: ', actual_response)\n",
    "    print('='*100, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def segment_one_sentence(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    seg_list = jieba.cut(sentence)\n",
    "\n",
    "    segments = ''\n",
    "    for word in seg_list:\n",
    "        segments = segments + ' ' + word\n",
    "        \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = '介紹一下你自己'\n",
    "input_X = segment_one_sentence(input_X)\n",
    "\n",
    "print('使用者說的話: ', input_X)\n",
    "print('聊天機器人的回應: ', summarizer.summarize(input_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "def segment_one_sentence(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    seg_list = jieba.cut(sentence)\n",
    "\n",
    "    segments = ''\n",
    "    for word in seg_list:\n",
    "        segments = segments + ' ' + word\n",
    "        \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = '介紹一下你自己'\n",
    "input_X = segment_one_sentence(input_X)\n",
    "\n",
    "print('使用者說的話: ', input_X)\n",
    "print('聊天機器人的回應: ', summarizer.summarize(input_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = '天氣有點熱'\n",
    "input_X = segment_one_sentence(input_X)\n",
    "\n",
    "print('使用者說的話: ', input_X)\n",
    "print('聊天機器人的回應: ', summarizer.summarize(input_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_X = '天氣有點熱'\n",
    "input_X = segment_one_sentence(input_X)\n",
    "\n",
    "print('使用者說的話:', input_X)\n",
    "print('聊天機器人的回應:', summarizer.summarize(input_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MAX_INPUT_SEQ_LENGTH = 10 # encoder端輸入序列的長度\n",
    "MAX_TARGET_SEQ_LENGTH = 15 # decoder端輸出序列的長度\n",
    "MAX_INPUT_VOCAB_SIZE = 1500 # encoder端的所有輸入(文本內的所有問句)的總詞數 (不重複)，。\n",
    "MAX_TARGET_VOCAB_SIZE = 1500 # decoder端的所有輸出(文本內的所有答句)的總詞數 (不重複)。\n",
    "\n",
    "\n",
    "def fit_text(X, Y, input_seq_max_length=None, target_seq_max_length=None):\n",
    "    if input_seq_max_length is None:\n",
    "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "    if target_seq_max_length is None:\n",
    "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "    input_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    max_input_seq_length = 0\n",
    "    max_target_seq_length = 0\n",
    "\n",
    "    for line in X:\n",
    "        text = [word.lower() for word in line.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > input_seq_max_length:\n",
    "            text = text[0:input_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            input_counter[word] += 1\n",
    "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "\n",
    "    for line in Y:\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        text = [word for word in line2.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > target_seq_max_length:\n",
    "            text = text[0:target_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            target_counter[word] += 1\n",
    "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
    "\n",
    "    input_word2idx = dict()\n",
    "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "        input_word2idx[word[0]] = idx + 2\n",
    "    input_word2idx['PAD'] = 0\n",
    "    input_word2idx['UNK'] = 1\n",
    "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "    target_word2idx = dict()\n",
    "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "        target_word2idx[word[0]] = idx + 1\n",
    "    target_word2idx['UNK'] = 0\n",
    "\n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "    \n",
    "    num_input_tokens = len(input_word2idx)\n",
    "    num_target_tokens = len(target_word2idx)\n",
    "\n",
    "    config = dict()\n",
    "    config['input_word2idx'] = input_word2idx\n",
    "    config['input_idx2word'] = input_idx2word\n",
    "    config['target_word2idx'] = target_word2idx\n",
    "    config['target_idx2word'] = target_idx2word\n",
    "    config['num_input_tokens'] = num_input_tokens\n",
    "    config['num_target_tokens'] = num_target_tokens\n",
    "    config['max_input_seq_length'] = max_input_seq_length\n",
    "    config['max_target_seq_length'] = max_target_seq_length\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "\n",
    "# 为语料做分词处理\n",
    "def word_segment():\n",
    "    # 打开语料文本\n",
    "    inputFile_NoSegment = open(os.getcwd() + '/對話文本/Happy.txt', 'rb')\n",
    "    outputFile_Segment = open(os.getcwd() + '/對話文本/Happy_segment.txt',\n",
    "                              'w', encoding='utf-8')\n",
    "\n",
    "    # 读取语料文本中的每一行文字\n",
    "    lines = inputFile_NoSegment.readlines()\n",
    "\n",
    "    # 为每一行文字分词\n",
    "    for i in range(len(lines)):\n",
    "        line = lines[i]\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            seg_list = jieba.cut(line)\n",
    "\n",
    "            segments = ''\n",
    "            for word in seg_list:\n",
    "                segments = segments + ' ' + word\n",
    "            segments += '\\n'\n",
    "            segments = segments.lstrip()\n",
    "\n",
    "            # 将分词后的语句，写进文件中\n",
    "            outputFile_Segment.write(segments)\n",
    "\n",
    "    inputFile_NoSegment.close()\n",
    "    outputFile_Segment.close()\n",
    "\n",
    "word_segment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 讀入斷詞後的文本\n",
    "text_Segment = open(os.getcwd() + '/對話文本/Happy_segment.txt',\n",
    "                    'r', encoding='utf-8')\n",
    "text_Segment_list = text_Segment.readlines()\n",
    "text_Segment.close()\n",
    "\n",
    "# 移除文本內的'\\n'\n",
    "text_Segment_list = [n.rstrip() for n in text_Segment_list]\n",
    "\n",
    "print('對話文本筆數：', len(text_Segment_list))\n",
    "print('對話文本內容：', text_Segment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "MAX_INPUT_SEQ_LENGTH = 10 # encoder端輸入序列的長度\n",
    "MAX_TARGET_SEQ_LENGTH = 15 # decoder端輸出序列的長度\n",
    "MAX_INPUT_VOCAB_SIZE = 1500 # encoder端的所有輸入(文本內的所有問句)的總詞數 (不重複)，若總數量超過設定值，則將多餘出現頻率較低的詞捨去。\n",
    "MAX_TARGET_VOCAB_SIZE = 1500 # decoder端的所有輸出(文本內的所有答句)的總詞數 (不重複)，若總數量超過設定值，則將多餘出現頻率較低的詞捨去。\n",
    "\n",
    "\n",
    "def fit_text(X, Y, input_seq_max_length=None, target_seq_max_length=None):\n",
    "    if input_seq_max_length is None:\n",
    "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "    if target_seq_max_length is None:\n",
    "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "    input_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    max_input_seq_length = 0\n",
    "    max_target_seq_length = 0\n",
    "\n",
    "    for line in X:\n",
    "        text = [word.lower() for word in line.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > input_seq_max_length:\n",
    "            text = text[0:input_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            input_counter[word] += 1\n",
    "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "\n",
    "    for line in Y:\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        text = [word for word in line2.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > target_seq_max_length:\n",
    "            text = text[0:target_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            target_counter[word] += 1\n",
    "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
    "\n",
    "    input_word2idx = dict()\n",
    "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "        input_word2idx[word[0]] = idx + 2\n",
    "    input_word2idx['PAD'] = 0\n",
    "    input_word2idx['UNK'] = 1\n",
    "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "    target_word2idx = dict()\n",
    "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "        target_word2idx[word[0]] = idx + 1\n",
    "    target_word2idx['UNK'] = 0\n",
    "\n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "    \n",
    "    num_input_tokens = len(input_word2idx)\n",
    "    num_target_tokens = len(target_word2idx)\n",
    "\n",
    "    config = dict()\n",
    "    config['input_word2idx'] = input_word2idx\n",
    "    config['input_idx2word'] = input_idx2word\n",
    "    config['target_word2idx'] = target_word2idx\n",
    "    config['target_idx2word'] = target_idx2word\n",
    "    config['num_input_tokens'] = num_input_tokens\n",
    "    config['num_target_tokens'] = num_target_tokens\n",
    "    config['max_input_seq_length'] = max_input_seq_length\n",
    "    config['max_target_seq_length'] = max_target_seq_length\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = text_Segment_list[0:][::2] # 使用者說的話\n",
    "Y = text_Segment_list[1:][::2] # 聊天機器人的回應\n",
    "config = fit_text(X, Y)\n",
    "\n",
    "print('num_input_tokens: ', config['num_input_tokens'])\n",
    "print('num_target_tokens: ', config['num_target_tokens'])\n",
    "print('max_input_seq_length: ', config['max_input_seq_length'])\n",
    "print('max_target_seq_length: ', config['max_target_seq_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "HIDDEN_UNITS = 100\n",
    "DEFAULT_BATCH_SIZE = 64\n",
    "VERBOSE = 1\n",
    "DEFAULT_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "class Seq2SeqSummarizer(object):\n",
    "\n",
    "    model_name = 'seq2seq'\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.num_input_tokens = config['num_input_tokens']\n",
    "        self.max_input_seq_length = config['max_input_seq_length']\n",
    "        self.num_target_tokens = config['num_target_tokens']\n",
    "        self.max_target_seq_length = config['max_target_seq_length']\n",
    "        self.input_word2idx = config['input_word2idx']\n",
    "        self.input_idx2word = config['input_idx2word']\n",
    "        self.target_word2idx = config['target_word2idx']\n",
    "        self.target_idx2word = config['target_idx2word']\n",
    "        self.config = config\n",
    "\n",
    "        self.version = 0\n",
    "        if 'version' in config:\n",
    "            self.version = config['version']\n",
    "\n",
    "        encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "        encoder_embedding = Embedding(input_dim=self.num_input_tokens, output_dim=HIDDEN_UNITS,\n",
    "                                      input_length=self.max_input_seq_length, name='encoder_embedding')\n",
    "        encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "        encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding(encoder_inputs))\n",
    "        encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "        decoder_inputs = Input(shape=(None, self.num_target_tokens), name='decoder_inputs')\n",
    "        decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "        decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
    "                                                                         initial_state=encoder_states)\n",
    "        decoder_dense = Dense(units=self.num_target_tokens, activation='softmax', name='decoder_dense')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        plot_model(model, to_file='model.png',show_shapes=True)\n",
    "        \n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=LEARNING_RATE), metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.encoder_model = Model(encoder_inputs, encoder_states)\n",
    "        plot_model(self.encoder_model, to_file='self.encoder_model.png',show_shapes=True)\n",
    "\n",
    "        decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "        plot_model(self.decoder_model, to_file='self.decoder_model.png',show_shapes=True)\n",
    "\n",
    "    def load_weights(self, weight_file_path):\n",
    "        if os.path.exists(weight_file_path):\n",
    "            self.model.load_weights(weight_file_path)\n",
    "\n",
    "    def transform_input_text(self, texts):\n",
    "        temp = []\n",
    "        for line in texts:\n",
    "            x = []\n",
    "            for word in line.lower().split(' '):\n",
    "                wid = 1\n",
    "                if word in self.input_word2idx:\n",
    "                    wid = self.input_word2idx[word]\n",
    "                x.append(wid)\n",
    "                if len(x) >= self.max_input_seq_length:\n",
    "                    break\n",
    "            temp.append(x)\n",
    "        temp = pad_sequences(temp, maxlen=self.max_input_seq_length)\n",
    "\n",
    "        print(temp.shape)\n",
    "        return temp\n",
    "\n",
    "    def transform_target_encoding(self, texts):\n",
    "        temp = []\n",
    "        for line in texts:\n",
    "            x = []\n",
    "            line2 = 'START ' + line.lower() + ' END'\n",
    "            for word in line2.split(' '):\n",
    "                x.append(word)\n",
    "                if len(x) >= self.max_target_seq_length:\n",
    "                    break\n",
    "            temp.append(x)\n",
    "\n",
    "        temp = np.array(temp)\n",
    "        print(temp.shape)\n",
    "        return temp\n",
    "\n",
    "    def generate_batch(self, x_samples, y_samples, batch_size):\n",
    "        num_batches = len(x_samples) // batch_size\n",
    "        while True:\n",
    "            for batchIdx in range(0, num_batches):\n",
    "                start = batchIdx * batch_size\n",
    "                end = (batchIdx + 1) * batch_size\n",
    "                encoder_input_data_batch = pad_sequences(x_samples[start:end], self.max_input_seq_length)\n",
    "                decoder_target_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length, self.num_target_tokens))\n",
    "                decoder_input_data_batch = np.zeros(shape=(batch_size, self.max_target_seq_length, self.num_target_tokens))\n",
    "                for lineIdx, target_words in enumerate(y_samples[start:end]):\n",
    "                    for idx, w in enumerate(target_words):\n",
    "                        w2idx = 0  # default [UNK]\n",
    "                        if w in self.target_word2idx:\n",
    "                            w2idx = self.target_word2idx[w]\n",
    "                        if w2idx != 0:\n",
    "                            decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
    "                            if idx > 0:\n",
    "                                decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "                yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def get_weight_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqSummarizer.model_name + '-weights.h5'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqSummarizer.model_name + '-config.npy'\n",
    "\n",
    "    @staticmethod\n",
    "    def get_architecture_file_path(model_dir_path):\n",
    "        return model_dir_path + '/' + Seq2SeqSummarizer.model_name + '-architecture.json'\n",
    "\n",
    "    def fit(self, Xtrain, Ytrain, Xtest, Ytest, epochs=None, batch_size=None, model_dir_path=None):\n",
    "        if epochs is None:\n",
    "            epochs = DEFAULT_EPOCHS\n",
    "        if model_dir_path is None:\n",
    "            model_dir_path = './models'\n",
    "        if batch_size is None:\n",
    "            batch_size = DEFAULT_BATCH_SIZE\n",
    "\n",
    "        self.version += 1\n",
    "        self.config['version'] = self.version\n",
    "        config_file_path = Seq2SeqSummarizer.get_config_file_path(model_dir_path)\n",
    "        weight_file_path = Seq2SeqSummarizer.get_weight_file_path(model_dir_path)\n",
    "        checkpoint = ModelCheckpoint(weight_file_path)\n",
    "        np.save(config_file_path, self.config)\n",
    "        architecture_file_path = Seq2SeqSummarizer.get_architecture_file_path(model_dir_path)\n",
    "        open(architecture_file_path, 'w').write(self.model.to_json())\n",
    "\n",
    "        Ytrain = self.transform_target_encoding(Ytrain)\n",
    "        Ytest = self.transform_target_encoding(Ytest)\n",
    "\n",
    "        Xtrain = self.transform_input_text(Xtrain)\n",
    "        Xtest = self.transform_input_text(Xtest)\n",
    "\n",
    "        train_gen = self.generate_batch(Xtrain, Ytrain, batch_size)\n",
    "        test_gen = self.generate_batch(Xtest, Ytest, batch_size)\n",
    "\n",
    "        train_num_batches = len(Xtrain) // batch_size\n",
    "        test_num_batches = len(Xtest) // batch_size\n",
    "\n",
    "        history = self.model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                           epochs=epochs,\n",
    "                                           verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                                           callbacks=[checkpoint])\n",
    "        self.model.save_weights(weight_file_path)\n",
    "        return history\n",
    "\n",
    "    def summarize(self, input_text):\n",
    "        input_seq = []\n",
    "        input_wids = []\n",
    "        for word in input_text.lower().split(' '):\n",
    "            idx = 1  # default [UNK]\n",
    "            if word in self.input_word2idx:\n",
    "                idx = self.input_word2idx[word]\n",
    "            input_wids.append(idx)\n",
    "        input_seq.append(input_wids)\n",
    "        input_seq = pad_sequences(input_seq, self.max_input_seq_length)\n",
    "        states_value = self.encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
    "        target_seq[0, 0, self.target_word2idx['START']] = 1\n",
    "        target_text = ''\n",
    "        target_text_len = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            #print (target_seq.shape)\n",
    "            #print (np.array(states_value).shape)\n",
    "            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n",
    "            #print (np.array(output_tokens).shape)\n",
    "            #print (np.array(h).shape)\n",
    "            #print (np.array(c).shape)\n",
    "\n",
    "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "            sample_word = self.target_idx2word[sample_token_idx]\n",
    "            target_text_len += 1\n",
    "\n",
    "            if sample_word != 'START' and sample_word != 'END':\n",
    "                target_text += ' ' + sample_word\n",
    "\n",
    "            if sample_word == 'END' or target_text_len >= self.max_target_seq_length:\n",
    "                terminated = True\n",
    "\n",
    "            target_seq = np.zeros((1, 1, self.num_target_tokens))\n",
    "            target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "            states_value = [h, c]\n",
    "        return target_text.strip()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
